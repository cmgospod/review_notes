1 Supervised learning requires labeled training data. Unsupervised learning does not.
2. Bias is underfitting, when you fail to account for the entire shape of the data. Variance is overfitting, when you start reading random noise as having predictive power.
3. Exploding gradients are a problem where very large error gradients accumulate and result in very large updates to neural network model weights. This can result in instability and inability to learn.
4. A confusion matrix is a 2x2 table listing true positives, false negatives, false positives, and true negatives.
6. A ROC curve is a visual representation of the rates of false positives and false negatives.
7. Selection bias is bias introduced by nonrandom sampling of the target population.
8. SVM is an algorithm attempting to create a hyperplane in n-space where n is the number of input variables, which divides the data as neatly as possible.
9. Support vectors are vectors drawn from data points to the hyperplane. 
10. Logistic regression is a linear regression of probability for being in each classification. It uses the sigmoid function to compress the linear regression into the domain (0,1)