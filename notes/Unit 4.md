Tokenize me
removing semantics, case, etc
WORD2VEC
stop words are super common and thus useless
stemming removes suffixes "ing" "s" etc
lemmatization is basically a fancier version of the same
document classification: use a grid search
neural networks